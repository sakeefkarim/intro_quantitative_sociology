{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMPkbxKQwP58HCSHNzdASJ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakeefkarim/intro_quantitative_sociology/blob/main/data/week%2011/code/SOCI269_Week_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://collegeaim.org/wp-content/uploads/2021/09/Amherst.png\" alt=\"Amherst Logo\" width=\"200\"/>\n",
        "\n",
        "\n",
        "# A *Very* Basic Introduction to $k$-Nearest Neighbours Algorithms in `Python` <img src=\"https://s3.dualstack.us-east-2.amazonaws.com/pythondotorg-assets/media/community/logos/python-logo-only.png\" alt=\"Python logo\" width=\"30\">\n",
        "\n",
        "[Sakeef M. Karim](https://www.sakeefkarim.com/)\n",
        "\n",
        "skarim@amherst.edu"
      ],
      "metadata": {
        "id": "ZYU4RHgj2iYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries\n",
        "\n",
        "Let's import a few essential libraries (e.g., `pandas` for data wrangling) and submodules (e.g., `sklearn.neighbors` from `scikit-learn`) to develop our $k$-nearest neighbours algorithm."
      ],
      "metadata": {
        "id": "XDvJheo527-2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jshN-B_C2dXr"
      },
      "outputs": [],
      "source": [
        "# For data manipulation:\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# From scikit-learn, we import modules to pre-process data, fit KNN classifier:\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# To save, load our model:\n",
        "\n",
        "from joblib import dump, load"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, we can programmatically mount our Google Drive folders onto a Colab session as follows:"
      ],
      "metadata": {
        "id": "-Vzk1v5M3Z8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "id": "OWt_pkKA20Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "To fit a supervised machine learning algorithm, we'll need some observed data. To this end, let's work with [gapminder](https://jennybc.github.io/gapminder/) once again."
      ],
      "metadata": {
        "id": "qGvz93o53fyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading gapminder dataset:\n",
        "\n",
        "gapminder = pd.read_excel('https://github.com/sakeefkarim/intro_quantitative_sociology/raw/refs/heads/main/data/week%2010/data/gapminder.xlsx')"
      ],
      "metadata": {
        "id": "rTK4OLvr3pki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Pre-Processing\" I"
      ],
      "metadata": {
        "id": "lrlXHIEY4Jug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the spirit of simplicity, we'll do some ***pre-processing*** by:\n",
        "\n",
        "+ Isolating the latest year in `gapminder` (2007) and dropping the `year` column.\n",
        "\n",
        "+ Generating a dummy indicator (`europe`) of whether a country is in Europe.\n",
        "\n",
        "+ Isolating our feature vector and target variable in separate objects."
      ],
      "metadata": {
        "id": "oX2Z44Im4Sru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Homing-in on observations in the latest year (2007)\n",
        "\n",
        "gapminder = gapminder.query('year == 2007').reset_index(drop = True).drop(columns='year')\n",
        "\n",
        "# Generating dummy indicator indexing whether a country is in Europe:\n",
        "\n",
        "gapminder['europe'] = pd.get_dummies(gapminder['continent'])['Europe']\n",
        "\n",
        "# Dropping observations with missing values (not necessary for gapminder):\n",
        "\n",
        "# gapminder.dropna(inplace = True)\n",
        "\n",
        "# Removing target variable and categorical indicators from feature vector:\n",
        "\n",
        "X = gapminder.drop(columns = ['europe', 'continent', 'country'])\n",
        "\n",
        "# Isolating target variable:\n",
        "\n",
        "y = gapminder['europe']"
      ],
      "metadata": {
        "id": "NBSh5X_n4Zlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Pre-Processing\" II: A Train-Test Split\n",
        "\n",
        "Next, we'll split our sample into two disjoint sets: a **training set** featuring 80% of our observations; and a **testing set**—or *hold-out sample* comprising 20% of the original dataset—that will not be involved in the training or validation process. We'll also ensure that our feature vectors have been standardized.\n"
      ],
      "metadata": {
        "id": "gjUOauih5b7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform train-test split:\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 905)\n",
        "\n",
        "# Standardizing feature data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Applied to training data\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Applied to test data\n",
        "\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "tEWRsuQH5bgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing KNN, Performing Cross-Validation\n",
        "\n",
        "Now, let's initialize our KNN and use (stratified) $k$-fold cross-validation to fit a basic KNN model."
      ],
      "metadata": {
        "id": "sarxsdrs6rqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing KNN classifier with k = 1, fitting model:\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors = 1)\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Stratified k-fold cross-validation:\n",
        "\n",
        "skfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 905)\n",
        "\n",
        "# Cross-validation score:\n",
        "\n",
        "print('Cross-Validation Score (Accuracy):', '{:.3f}'.format(cross_val_score(knn, X_train, y_train, cv = skfold).mean()))\n",
        "\n",
        "# Measure of predictive performance:\n",
        "\n",
        "print('Test Score (Accuracy):', '{:.3f}'.format(knn.score(X_test, y_test)))\n",
        "\n",
        "# Predictions on hold-out subsample\n",
        "\n",
        "europe_pred = knn.predict(X_test)\n",
        "\n",
        "# More performance metrics\n",
        "\n",
        "print(classification_report(y_test, europe_pred))\n"
      ],
      "metadata": {
        "id": "SZPu0iXa62FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Optimization\n",
        "\n",
        "Next, we'll use the `GridSearchCV` method to select the optimal value of $k$ by using a grid search of possible hyperparameter values (odd numbers between 1 and 13)."
      ],
      "metadata": {
        "id": "Mt8tb2i99RIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a grid of potential hyperparameter values (odd numbers from 1 to 13):\n",
        "\n",
        "k_grid = {'n_neighbors': np.arange(start = 1, stop = 14, step = 2) }\n",
        "\n",
        "# Setting up a grid search to home-in on best value of k:\n",
        "\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid = k_grid, cv = skfold)\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Extract best score and hyperparameter value:\n",
        "\n",
        "print(f'Best Mean Cross-Validation Score: {grid.best_score_:.3f}')\n",
        "\n",
        "print(f'Best Parameter (Value of k): {grid.best_params_['n_neighbors']}')\n",
        "\n",
        "print(f'Test Set Score: {grid.score(X_test, y_test):.3f}')\n",
        "\n",
        "\n",
        "# pd.DataFrame(grid.cv_results_)"
      ],
      "metadata": {
        "id": "uiL7A0799Xix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing Model for Future Use\n",
        "\n",
        "Finally, we'll generate our $k_9$ model and store it for future use."
      ],
      "metadata": {
        "id": "OjYHzKx5DIe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model of choice in Google Drive folder:\n",
        "\n",
        "dump(grid.best_estimator_, '/drive/My Drive/Colab Notebooks/knn_classifier.joblib')\n",
        "\n",
        "# Using it in the future:\n",
        "\n",
        "# loaded_knn = load('/drive/My Drive/Colab Notebooks/knn_classifier.joblib')\n",
        "\n",
        "# loaded_knn.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "QInhdf_FDPIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises\n",
        "\n",
        "1. Import the `penguins` data frame from the [`{palmerpenguins}`](https://allisonhorst.github.io/palmerpenguins/) package into `Python`.\n",
        "\n",
        "2. Isolate observations from the latest `year` in `penguins`.\n",
        "\n",
        "3. Develop a $k$-nearest neighbours **regressor** to predict a numeric outcome of interest. Report your algorithm's cross-validation score and out-of-sample performance.\n",
        "\n",
        "4.  Develop a $k$-nearest neighbours **classifier** to predict a categorical outcome of interest. Report your algorithm's cross-validation score and out-of-sample performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "qgIVAlU3EpIL"
      }
    }
  ]
}